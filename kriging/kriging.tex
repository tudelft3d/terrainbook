%!TEX root = ../terrainbook.tex

\graphicspath{{kriging/}}
% \setchapterpreamble[u]{\margintoc}
\setchapterpreamble[u]{\youtube[0pt]{youtu.be/ZaadzBvgE2s}\margintoc[18pt]}


\chapter{Spatial interpolation: kriging}%
\label{chap:kriging}

Kriging is a spatial interpolation method that was developed mostly by Georges Matheron based on the earlier work of Danie Krige, who created it to estimate the yield of gold mines in South Africa.
In contrast to other spatial interpolation methods, it involves creating a custom model that is fine-tuned using the statistical properties of each dataset.
In this way, kriging can take into account the specific characteristics of a dataset, often yielding better results than other interpolation methods.

Like other techniques based on geostatistical models, kriging relies on the fact that when one moves across space, values such as the gold content in rock or the elevation in a terrain have both a general spatial \emph{trend}\index{trend} (\eg\ a flat mean value, a fitted plane or a more complex polynomial defining a surface) and a certain spatially correlated \emph{randomness} (\ie\ closer points tend to have more similar values).
Both of these elements are modelled in kriging.

More than a single method, kriging comprises a family of related methods.
Within this chapter, we will look at two related types of kriging: simple kriging and ordinary kriging.
These treat the spatially correlated randomness in a similar way, but they make different assumptions about the trend in a dataset.

\section{Statistical background}

The physical processes that shape the world can be considered to be at least partly deterministic.
In the case of a terrain, the elevation is determined by processes that we can model (more or less accurately), such as plate tectonics, volcanic activity, and erosion.
However, these processes are too complex and not understood well enough to use them to obtain accurate elevation values.
Imagine, for instance, how difficult it would be to get an accurate elevation map of the world using only the shape of the tectonic plates and some other parameters (\eg\ their direction and speed of movement).

Because of this complexity, the value of complex properties, such as the elevation of a terrain, are usually treated in geostatistics as the result of what is known as a \emph{random}\marginnote{random process}\index{random process} or \emph{stochastic process}\marginnote{stochastic process}\index{stochastic process}.
In this context, randomness can be understood as the fact that the value of a property at an unsampled location is not known exactly, and so we cannot assign it an exact number.
Instead, we can make an educated guess of the value at that location by creating a statistical model of its possible values using a \emph{probability distribution}\marginnote{probability distribution}\index{probability distribution}, which we can associate with a function (\ie\ a \emph{probability distribution function}) or with a set of standard statistical measures, such as the mean and variance.

This situation is phrased in mathematical terms by saying that the value of the elevation property \(Z\) at a location \(x\) is a \emph{random variable}\marginnote{random variable}\index{random variable} \(Z(x)\).
For the sake of simplicity, we will usually omit the location and denote it just as \(Z\); or when working with multiple locations (\eg\ \(x_i\) and \(x_j\)), we will shorten their respective random variables (\(Z(x_i)\) and \(Z(x_j)\)) using subscripts (\(Z_i\) and \(Z_j\)).

In geostatistics, the most common way to express the general shape of the probability distribution of a random variable is in terms of its mean and its variance.
Here, the mean\marginnote{mean}\index{mean} (also called \emph{expectation}\marginnote{expectation}\index{expectation} or \emph{expected value}\marginnote{expected value}\index{expected value}) of a random variable \(Z\) is a sort of probability-weighted average of its possible values and is denoted as \(E[Z]\) or \(\mu\).
Meanwhile, the \emph{variance}\marginnote{variance}\index{variance} of a random variable \(Z\) is a measure of how far the values of \(Z\) will usually spread from its mean, and it is denoted as \(\mathrm{var}(Z)\) or \(\sigma^2\).
A small variance thus means that a few random samples of \(Z\) at nearby locations will likely form a tight cluster around its mean, whereas a large variance will have sample values that are more distant from the mean value.

Mathematically, the variance is defined as the expected value of the squared deviation from the expected value of \(Z\), or:

\begin{align}
\mathrm{var}(Z) &= E\left[{\left(Z-E\left[Z\right]\right)}^2\right] \label{eq:variance1} \\
&= E\left[Z^2 -2ZE\left[Z\right] + E[Z]^2\right] \nonumber \\
&= E\left[Z^2\right] - 2E\left[Z\right]E\left[Z\right] + E\left[Z\right]^2 \nonumber \\
&= E\left[Z^2\right] - 2E\left[Z\right]^2 + E\left[Z\right]^2 \nonumber \\
&= E\left[Z^2\right]-{E[Z]}^2. \label{eq:variance2}
\end{align}

Next, it is important to define the covariance\marginnote{covariance}\index{covariance}, denoted as \(\mathrm{cov}(Z_i,Z_j)\), or \(\sigma_{ij}\), which expresses the joint variability of the random variables \(Z_i\) and \(Z_j\).
Thus, a positive covariance between \(Z_i\) and \(Z_j\) means that when one increases/decreases, the other is expected to increase/decrease in the same direction.
Conversely, a negative covariance means that the variables tend to increase/decrease in opposite directions.
The magnitude of the covariance is related to the magnitude of this increase or decrease.
It is thus defined mathematically as the expected product of their deviations from their (individual) expected values, or:

\begin{align}
\mathrm{cov}(Z_i, Z_j) &= E\left[\left(Z_i-E[Z_i]\right) \left(Z_j-E[Z_j]\right)\right] \label{eq:covariance} \\
&= E\left[Z_iZ_j - Z_iE\left[Z_j\right] - E\left[Z_i\right]Z_j + E\left[Z_i\right]E\left[Z_j\right]\right] \nonumber \\
&= E\left[Z_iZ_j\right] - E\left[Z_i\right]E\left[Z_j\right] - E\left[Z_i\right]E\left[Z_j\right] + E\left[Z_i\right]E\left[Z_j\right] \nonumber \\
&= E\left[Z_iZ_j\right] - E\left[Z_i\right]E\left[Z_j\right]. \label{eq:covariance2}
\end{align}

Here, it is good to note that the covariance of \(Z_i\) with itself is equivalent to its variance:

\begin{equation}
\mathrm{cov}(Z,Z) = E\left[\left(Z-E[Z]\right)^2\right] = \mathrm{var}(Z). \nonumber
\end{equation}

While not used further in this lesson, it is also good to know that the variance and the covariance can be used to calculate the Pearson correlation coefficient\marginnote{correlation coefficient}\index{correlation coefficient} \(\rho_{ij}\), which is one of the most common statistical measures that is applied to datasets:

\begin{equation}
\rho_{ij}=\frac{\mathrm{cov}(Z_i, Z_j)}{\sqrt{\mathrm{var}(Z_i) \mathrm{var}(Z_j)}}. \nonumber
\end{equation}

Note that this is essentially just a normalised form of the covariance.

\section{Geostatistics and the standard geostatistical model}[Geostatistical model]

In geostatistics, we apply the concepts covered in the previous section on general-purpose statistics to consider how they work with spatial phenomena, where values have a location and are often \emph{spatially} correlated (not just correlated).

The model most commonly applied in geostatistics considers that a random variable \(Z\), which represents a spatially correlated property at a given location, can be decomposed into two related variables: (i) a non-random spatial trend that can be modelled by the expectation \(E[Z]\) (\eg\ using a constant, a polynomial, a spline, etc.); and (ii) a random but spatially correlated deviation from this trend that is considered as a sort of adjustment, error or residual term and is here denoted as \(R\).
In the case of elevation, the former would represent the general shape of the terrain, whereas the latter would represent local differences from it.
We therefore have:

\begin{equation}
\label{eq:geostat}
Z = E\left[Z\right] + R.
\end{equation}

The different types of kriging from this chapter model the trend in a different way but treat the residual term in a similar way.
These are:
\begin{itemize}
\item \emph{simple kriging}, where the trend is a known constant that we specify in the model; and
\item \emph{ordinary kriging}, where the trend is a local mean that we calculate in the interpolation process.
\end{itemize}

These will be described in detail later in the chapter.
However, in order to understand how these work and when they can be applied, it is important to cover some common properties of the two terms of the standard geostatistical model.

Regarding the expectation/trend, simple kriging relies on the assumption that the expectation \(E(Z)\) is the same everywhere, which is known as the \emph{stationarity of the mean}\marginnote{stationarity of the mean}\index{stationarity of the mean}.
In the case of a terrain, that could mean that a terrain is uneven with significant peaks and valleys, but that there is not a general trend across it (\eg\ a clear slope with higher elevations on one side and lower elevations on the opposite side).
Mathematically, we can express that as:

\begin{align}
\label{eq:stationarityofthemean}
E\left[Z(x+h\right)] = E\left[Z(x)\right],
\end{align}

where \(x\) is an arbitrary point in the domain (\ie\ the area we want to interpolate), \(h\) is any vector from \(x\) to another point in the domain and \(Z(x)\) is the value of a random variable at \(x\) (\eg\ its elevation).

Next, there are also important properties of the residual term.
First, note that since \(R = Z - E[Z] \), the variance (Equation~\ref{eq:variance1}) and covariance (Equation~\ref{eq:covariance}) can be defined in a simple way in terms of the residuals:

\begin{align}
\mathrm{var}\left(Z\right) &= E\left[{R}^2\right], \label{eq:varres} \\
\mathrm{cov}(Z_i,Z_j) &= E\left[R_i \cdot R_j\right]. \label{eq:covres}
\end{align}

Then, since we know that the expectation is by definition the mean, in order not to introduce any bias, the expected value of the residual must be zero, \ie\ \(E\left[R\right] = 0\).
When this is fulfilled, the model is said to be \emph{unbiased}\marginnote{unbiased}\index{unbiased}.
The way that is is achieved varies in different types of kriging.

Finally, another common assumption is that the residual term at a pair of locations does not depend on the locations, but instead can be defined based only on the vector separating them.
In the case of a terrain, this would mean that the likelihood of finding similar elevations at two points separated by a given distance and orientation does not change across the terrain.
For instance, a terrain that goes from smooth on one side to rough on the other would not satisfy this assumption.
Mathematically, we can express this using the covariance as:

\begin{align}
\label{eq:stationarityofthecovariance}
\mathrm{cov}\left(Z(x+h), Z(x)\right) = C(h),
\end{align}

where \(C\) is the covariance function.
Since both the expectation (Equation~\ref{eq:stationarityofthemean}) and the covariance (Equation~\ref{eq:stationarityofthecovariance}) are translation invariant, this pair of assumptions are together known as \emph{second-order stationarity}\marginnote{second-order stationarity}\index{second-order stationarity}.

\section{Covariance, dissimilarity and the semivariogram}%
\index{variogram}

According to the standard geostatistical model described above, the residual term is spatially correlated.
That is, even after removing the general spatial trend from a dataset, nearby samples will tend to have more similar values than those farther apart.
In kriging, we exploit that property by modelling it through a \emph{semivariogram} or a \emph{covariance function}.
These are roughly opposites, since the semivariogram is a measure of dissimilarity and the covariance is a measure of similarity.
However, both attempt to measure how much spatial correlation there is as a function of distance and both can be used with kriging.

The semivariogram \(\gamma(h)\)\marginnote{semivariogram}\index{semivariogram}, often just called a variogram\marginnote{variogram}\index{variogram} for short, is a function that expresses the average dissimilarity of the value of a random variable \(Z\) between sample points at different distances.
It is defined as:

\begin{equation}
\label{eq:semivariogram}
\gamma(h) = \frac{1}{2} (Z(x+h) - Z(x))^2,
\end{equation}

where \(x\) is a sample point, \(h\) is a vector from \(x\) to another sample point and \(Z(x)\) is the value of a random variable at \(x\) (\eg\ its elevation).
Note that the `semi' in semivariogram comes from the \(1/2\) in Equation~\ref{eq:semivariogram}.

When this is done with every possible pair of sample points in a dataset, or with a representative subset in order to speed up the process as it is usually done in practice, \(|h|\) (\ie\ the magnitude of the vector \(h\)) and \(\gamma(h)\) can be put into a scatter plot to show how the average dissimilarity of a value changes with the distance between the sample points.
The result of such a plot is what is known as a \emph{variogram cloud}\marginnote{variogram cloud}\index{variogram cloud} (Figure~\ref{fig:variogram_cloud}).

\begin{figure*}[htbp]
\begin{subfigure}{0.5\linewidth}
\centering
\includegraphics[width=\linewidth]{figs/data}
\caption{dataset}
\end{subfigure}%
\begin{subfigure}{0.5\linewidth}
\centering
\includegraphics[width=\linewidth]{figs/variogram_cloud}
\caption{variogram cloud}
\end{subfigure}%
\caption{Starting from (a) a sample dataset, (b) the variogram cloud can be computed.
In this case, only 1\% randomly selected point pairs were used.}%
\label{fig:variogram_cloud}
\end{figure*}

In this figure, it is possible to see some typical characteristics of a variogram cloud.
Since nearby sample points tend to have similar values, the dissimilarity tends to increase as the distance between sample points increases.
However, it is worth noting that since the farthest away pairs of sample points have similar values in this specific dataset, the dissimilarity also decreases at the highest distances.

Since most of the time there is a wide variation between the dissimilarities shown at all distances in a variogram cloud, the next step is to average the dissimilarity of the pairs of sample points based on distance intervals.
Mathematically, a series of averages of dissimilarities \(\gamma^\star(h)\), known as \emph{experimental semivariances}\marginnote{experimental semivariance}\index{experimental semivariance}, can be created by computing the average dissimilarities for all vectors whose lengths are within a series of specified intervals (generally known as \emph{bins} or \emph{lags}).
\marginnote{intervals (bins)}
Given a set \(\mathfrak{h}\) containing the vectors for a distance interval, the experimental semivariances are computed as:

\begin{equation}
\gamma^\star(\mathfrak{h}) = \frac{1}{2n}\sum\left(z\left(x+h\right)-z\left(x\right)\right)^2 \hspace{1cm}\text{for all } h \in \mathfrak{h}
\end{equation}

where \(n\) is the number of sample point pairs in \(\mathfrak{h}\).

This computation results in much smoother values for the dissimilarity, and the values of \(\gamma^\star(h)\) for all values of \(|h|\) are known as an \emph{experimental}\marginnote{experimental variogram}\index{experimental variogram} or \emph{empirical variogram}\marginnote{empirical variogram}\index{empirical variogram} (Figure~\ref{fig:experimental_variogram}a).
Figure~\ref{fig:experimental_variogram}b shows the relationship between the experimental semivariances, covariances and variance.
\begin{figure*}[htbp]
\begin{subfigure}{0.5\linewidth}
\centering
\includegraphics[width=\linewidth]{figs/experimental_variogram}
\caption{experimental variogram}
\end{subfigure}%
\begin{subfigure}{0.5\linewidth}
\centering
\includegraphics[width=\linewidth]{figs/semivariance_covariance}
\caption{parameters}
\end{subfigure}%
\caption{(a) The experimental variogram and (b) the relationship between the experimental semivariances, covariances and variance.}%
\label{fig:experimental_variogram}
\end{figure*}

Note that in order to avoid the unreliable dissimilarities that are common at large distances between sample points, it is usual practice to only compute the experimental variogram for distances up to about half of the size of the region covered by the dataset.

From the scatterplot of the experimental variogram, it is possible to see how a few important parameters can be used to describe it (Figure~\ref{fig:example_variogram}): 
\begin{itemize}
  \item the \emph{sill}\marginnote{sill}\index{sill}, which is the upper bound of \(\gamma^\star(h)\);
  \item the \emph{range}\marginnote{range}\index{range}, which is the value of \(|h|\) when it converges; 
  \item the \emph{nugget}\marginnote{nugget}\index{nugget}, which is the value of \(\gamma^\star(h)\) when \(|h|\) approaches 0.
\end{itemize}
\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{figs/example_variogram}
\caption{An experimental variogram can be described in terms of a few parameters.}%
\label{fig:example_variogram}
\end{figure}

Finally, the last step is to use these parameters to replace the experimental variogram with a \emph{theoretical variogram function}\marginnote{theoretical variogram function}\index{theoretical variogram function} that approximates it and which can be more easily evaluated for further calculations.
Depending on the shape of the variogram, there are various functions that can be used.
Some examples are:

\begin{align}
\gamma_\mathrm{circular}(h) &= \begin{cases}
  s \left(1-\frac{2}{\pi}\cos^{-1}\left(\frac{|h|}{r}\right)+\frac{2|h|}{\pi r}\sqrt{1-\frac{|h|^2}{r^2}}\right) + n & \text{if } |h| \leq r \\
  s + n & \text{if } |h| > r
  \end{cases}\\
\gamma_\mathrm{cubic}(h) &= \begin{cases}
  s \left(\frac{7|h|^2}{r^2}-\frac{8.75|h|^3}{r^3}+\frac{3.5|h|^5}{r^5}-\frac{0.75|h|^7}{r^7}\right) + n & \text{if } |h| \leq r \\
  s + n & \text{if } |h| > r
  \end{cases}\\
\gamma_\mathrm{exponential}(h) &= s \left(1 - e^\frac{-3|h|}{r}\right) + n \\
\gamma_\mathrm{gaussian}(h) &= s \left(1 - e^\frac{-\left(3|h|\right)^2}{r^2}\right) + n \\
\gamma_\mathrm{linear}(h) &= \begin{cases}
  \frac{s|h|}{r} + n & \text{if } |h| \leq r \\
  s + n & \text{if } |h| > r
  \end{cases}\\
\gamma_\mathrm{power}(h) &= \begin{cases}
  \frac{sr^2}{|h|^2} + n & \text{if } |h| \leq r \\
  s + n & \text{if } |h| > r
  \end{cases}\\
\gamma_\mathrm{spherical}(h) &= \begin{cases} 
   s \left(\frac{3|h|}{2r} - \frac{|h|^3}{2r^3}\right) + n & \text{if } |h| \leq r \\
   s + n & \text{if } |h| > r
  \end{cases}
\end{align}

where \(s\) is the \emph{sill}, set to roughly the value of \(\gamma^\star(h)\) when \(\gamma^\star(h)\) is  flat; \(r\) is the \emph{range}, roughly the minimum value of \(|h|\) where \(\gamma^\star(h)\) is flat, and \(n\) is the nugget, which is the starting value of \(\gamma^\star(h)\).
Figure~\ref{fig:theoretical_variogram} shows the result of fitting the example theoretical variogram functions.
Note how the cubic and especially the Gaussian functions are a good fit in this case.

\begin{figure*}[htbp]
\begin{subfigure}{0.33\linewidth}
\centering
\includegraphics[width=\linewidth]{figs/model_circular}
\caption{Circular}
\end{subfigure}%
\begin{subfigure}{0.33\linewidth}
\centering
\includegraphics[width=\linewidth]{figs/model_cubic}
\caption{Cubic}
\end{subfigure}%
\begin{subfigure}{0.33\linewidth}
\centering
\includegraphics[width=\linewidth]{figs/model_exponential}
\caption{Exponential}
\end{subfigure}\\
\begin{subfigure}{0.33\linewidth}
\centering
\includegraphics[width=\linewidth]{figs/model_gaussian}
\caption{Gaussian}
\end{subfigure}%
\begin{subfigure}{0.33\linewidth}
\centering
\includegraphics[width=\linewidth]{figs/model_linear}
\caption{Linear (bounded)}
\end{subfigure}%
\begin{subfigure}{0.33\linewidth}
\centering
\includegraphics[width=\linewidth]{figs/model_power}
\caption{Power (bounded)}
\end{subfigure}\\
\begin{subfigure}{0.33\linewidth}
\centering
\includegraphics[width=\linewidth]{figs/model_spherical}
\caption{Spherical}
\end{subfigure}%
\caption{Some possible theoretical variogram functions}%
\label{fig:theoretical_variogram}
\end{figure*}

Many other theoretical variogram functions are possible, \eg\ circular, cubic, linear, etc.
However, the three described above are the most common found in practice.

Before moving on to apply these to kriging, there are a couple of important points.
First, note that these theoretical functions are often only applied when \(|h| > 0\), since setting \(\gamma(0) = 0\) helps to ensure that kriging passes exactly through the sample points (the exact property as explained in Section~\ref{sec:interpol_properties}).
Second, all of the semivariogram-related functions seen in this section can be converted to covariance functions as well, taking into account that \(\gamma(h) = \mathrm{sill} - C(h)\).
Note that this means that the covariance is high when \(|h|\) is small and it decreases as \(|h|\) increases.

\section{Simple kriging}

Simple kriging is similar to other spatial interpolation methods that use a weighted average.
It starts from the assumption of second-order stationarity.
Moreover, the expectation is also known, and so the general procedure to perform it is to: (i) subtract it from the sample points to obtain residuals, (ii) use the residuals to define a function that estimates the value of the residual term at any location, and (iii) interpolate at the desired locations using the function added to the expectation.

Thus, simple kriging defines a function \(\hat{R}_0\) that estimates the value of the residual \(R\) of the random variable \(Z\) at a location \(x_0\) as a weighted average of its residuals at the \(n\) sample points \(x_i\) that we will use for the interpolation (where \(1 \leq i \leq n\)).
We denote this as:

\begin{equation}
\label{eq:wask}
\hat{R}_0 = \hat{Z}_0 - E[Z_0] = \sum_{i=1}^n w_i (\underbrace{Z_i-E[Z_i]}_{R_i}).
\end{equation}

Simple kriging is unbiased, and therefore the expected value of the estimation at a location \(x_0\) is equal to the expected value at that location.
In mathematical terms, we can formulate this as:

\begin{equation}
\label{eq:unbiased}
E\left[ \hat{Z}_0 - Z_0 \right] = 0 \hspace{1cm}\text{or}\hspace{1cm}E\left[Z_0\right] = E\left[ \hat{Z}_0 \right].
\end{equation}

% In order to check this for simple kriging, we can put the weighted average from Equation~\ref{eq:wask} in this equation, which results in the following:

% % TODO : why is latex complaining at \end{align} I removed \cancelto{} and it's solved
% \begin{align}
%   E\left[ Z_0 \right] &= E\left[ E[Z_0] + \sum_{i=1}^n w_i R_i \right] \nonumber \\
%   &= E[Z_0] + \sum_{i=1}^n w_i 0{E[R_i]} \nonumber \\
%   &= E[Z_0].  \nonumber
% \end{align}

Then, in order to derive the equations used in simple kriging, we start from the fact that it \emph{minimises the variance of the estimation error}\marginnote{minimisation of the variance}\index{minimisation of the variance}, which in this case is given by \(\mathrm{var}\left(\hat{R}_0 - R_0\right)\).
If we use the definition of the variance from Equation~\ref{eq:variance1}, this can be instead put in terms of an expectation:

\begin{equation}
\mathrm{var}\left(\hat{R}_0 - R_0\right) = E \left[ \left( \left(\hat{R}_0 - R_0\right) - E \left[\hat{R}_0 - R_0\right] \right)^2 \right] \nonumber
\end{equation}

However, we know from the unbiased criterion from Equation~\ref{eq:unbiased} that \(E\left[ \hat{R}_0 - R_0 \right] = 0\), and so we can simplify the previous equation as:

\begin{equation}
\mathrm{var}\left(\hat{R}_0 - R_0\right) = E\left[\left( \hat{R}_0 - R_0 \right)^2\right] \nonumber.
\end{equation}

If this is expanded, it results in:

\begin{align}
\mathrm{var}\left(\hat{R}_0 - R_0\right) &= E\left[{\hat{R}_0}^{\hspace{1.2mm}2} - 2\hat{R}_0R_0 + {R_0}^2 \right] \nonumber \\
&= E\left[{\hat{R}_0}^{\hspace{1.2mm}2}\right] - 2E\left[\hat{R}_0R_0\right] + E\left[{R_0}^2 \right] \nonumber \\
&= E\left[\sum_{i=1}^n \sum_{j=1}^n w_i w_j R_i R_j\right] - 2E\left[\sum_{i=1}^n w_i R_i R_0\right] + E\left[{R_0}^2 \right] \nonumber \\
&= \sum_{i=1}^n \sum_{j=1}^n w_i w_j E\left[R_i R_j\right] - 2\sum_{i=1}^n w_i E\left[R_i R_0\right] + E\left[{R_0}^2 \right]. \nonumber
\end{align}

Here, we can use the definitions of the variance based on residuals from Equations~\ref{eq:varres} and~\ref{eq:covres} together with our covariance formula from Equation~\ref{eq:stationarityofthecovariance}, which yields:

\begin{align}
\mathrm{var}\left(\hat{R}_0 - R_0\right) &= \sum_{i=1}^n \sum_{j=1}^n w_i w_j \mathrm{cov}(R_i, R_j) - 2\sum_{i=1}^n w_i \mathrm{cov}(R_i,R_0) + \mathrm{cov}(R_0, R_0) \label{eq:variancesk} \\
&= \sum_{i=1}^n \sum_{j=1}^n w_i w_j C(x_i-x_j) - 2\sum_{i=1}^n w_i C(x_i-x_0) + C(x_0-x_0).
\end{align}

In order to minimise this equation, we can find where its first derivative is zero.
This is:

\begin{equation}
\frac{\partial \mathrm{var}\left(\hat{R}_0 - R_0\right)}{\partial w_i} = 2 \sum_{j=1}^n w_j C(x_i-x_j) - 2C(x_i-x_0) = 0 \hspace{1cm} \text{for all } 1 \leq i \leq n, \nonumber
\end{equation}

which yields the set of \(n\) simple kriging equations:

\begin{equation}
\sum_{j=1}^n w_j C(x_i-x_j) = C(x_i-x_0).
\end{equation}

While these equations can be used to perform simple kriging, it is often easier to deal with these in matrix form:

\begin{equation}
\underbrace{\left(\begin{array}{c}
w_1 \\
\vdots \\
w_n \end{array} \right)}_{w}
%
\underbrace{\left( \begin{array}{ccc}
C(x_1-x_1) & \cdots & C(x_1-x_n) \\
\vdots & \ddots & \vdots \\
C(x_n-x_1) & \cdots & C(x_n-x_n) \end{array} \right)}_{A}
 = 
%
\underbrace{\left(\begin{array}{c}
C(x_1-x_0) \\
\vdots \\
C(x_n-x_0) \end{array} \right)}_{d}
\end{equation}

which is known as the \emph{simple kriging system}\marginnote{simple kriging system}\index{simple kriging system}.
Finally, if we invert the matrix \(A\), the weights are given by:

\begin{equation}
w = A^{-1}d.
\end{equation}

These weights can be applied to interpolate the value of the residual term at any location as a weighted average of the sample points, where the correlation between the points is given by a covariance function, which can be obtained from the semivariogram.

However, simple kriging does not tell us what value we should use for the expectation \(E[Z]\), since we start from the assumption that it is known, which is often not the case.
Even a seemingly reasonable value, such as average of all points, can be very inaccurate if the sample points are unevenly distributed across the domain.

\section{Ordinary kriging}%
\index{ordinary kriging}

Ordinary kriging is similar to simple kriging in that it estimates values using a weighted average function with weights computed from the semivariogram/covariance based on the distance between the points. 
However, it estimates a value at a location using only sample points in the neighbourhood of the location and relies only on \emph{local} second-order stationarity, \ie\ a constant mean within a moving neighbourhood, with a mean that is computed within the method itself.

Rather than relying on the residuals, it defines a function \(\hat{Z}_0\) that directly estimates the value of the random variable \(Z\) at a location \(x_0\) as a weighted average of its value at the \(n\) neighbouring sample points \(x_i\) that we will use for the interpolation (where \(1 \leq i \leq n\)).
We denote this as:

\begin{equation}
\label{eq:waok}
\hat{Z}_0 = \sum_{i=1}^n w_i Z_i.
\end{equation}

Like simple kriging, ordinary kriging is \emph{unbiased}, which is achieved by making sure that the interpolation weights add up to one, \ie\ \(\sum_{i=1}^n w_i = 1\).
Ordinary kriging also \emph{minimises the variance of the estimation error}, which is given by \(\mathrm{var}\left(\hat{Z}_0 - Z_0\right)\).
For this, we can use the same derivation as for simple kriging up to Equation~\ref{eq:variancesk} but using the variogram for the final step.
This is:

\begin{align}
\mathrm{var}\left(\hat{R}_0 - R_0\right) &= \sum_{i=1}^n \sum_{j=1}^n w_i w_j \mathrm{cov}(R_i, R_j) - 2\sum_{i=1}^n w_i \mathrm{cov}(R_i,R_0) + \mathrm{cov}(R_0, R_0) \nonumber \\
&= -\sum_{i=1}^n \sum_{j=1}^n w_i w_j \gamma(x_i-x_j) + 2\sum_{i=1}^n w_i \gamma(x_i-x_0) - \gamma(x_0-x_0).
\end{align}

Using the previous equation and the unbiased criterion from Equation~\ref{eq:unbiased}, we can apply the minimisation method known as Lagrange multipliers\footnote{\url{https://en.wikipedia.org/wiki/Lagrange_multiplier}} and arrive at the set of \(n+1\) ordinary kriging equations:

\begin{align}
\sum_{j=1}^n w_i \gamma(x_i-x_j) + \mu(x_0) &= \gamma(x_i - x_0) \hspace{1cm} \text{for all } 1 \leq i \leq n \nonumber \\
\sum_{j=1}^n w_i &= 1 
\end{align}

where \(\mu(x_0)\) is a Lagrange parameter that was used in the minimisation process.

Like with simple kriging, these equations can be used to perform ordinary kriging, but it is often easier to deal with these in matrix form:

\begin{equation}
%
\underbrace{\left( \begin{array}{cccc}
\gamma(x_1-x_1) & \cdots & \gamma(x_1-x_n) & 1 \\
\vdots & \ddots & \vdots & 1 \\
\gamma(x_n-x_1) & \cdots & \gamma(x_n-x_n) & 1 \\
1 & \cdots & 1 & 0 \end{array} \right)}_{A}
%
\underbrace{\left(\begin{array}{c}
w_1 \\
\vdots \\
w_n \\
\mu(x_0) \end{array} \right)}_{w} = 
%
\underbrace{\left(\begin{array}{c}
\gamma(x_1-x_0) \\
\vdots \\
\gamma(x_n-x_0) \\
1 \end{array} \right)}_{d}
\end{equation}

which is known as the \emph{ordinary kriging system}\marginnote{ordinary kriging system}\index{ordinary kriging system}.

Finally, if we invert the matrix \(A\), the weights and the Lagrange multipliers are given by:

\begin{equation}
w = A^{-1}d
\end{equation}

%%%
%
\section{Other types of kriging}

\begin{description}
\item[Directional kriging] is useful when the similarity of points differs according to different directions, \eg\ north-south versus east-west.
It involves creating variograms for different directions.
\item[Block kriging] attempts to obtain a local trend for an area around a point (rather than just at a point).
It can be used to obtain smoother results.
\item[Cokriging] applies kriging to multiple correlated variables.
It is particularly useful when you want to interpolate one variable with limited sample points but there are more sample points for another correlated variable.
\item[Indicator kriging] applies thresholds to obtain discrete values, such as presence/absence of a variable or a set of distinct classification classes.
\item[Poisson kriging] applies kriging to data involving cumulative counts or rates.
It is often applied together with polygonal datasets.
\item[Universal kriging] fits the trend using a predefined deterministic function.
It is tricky to use in practice because it imposes conditions on the underlying variogram.
\end{description}

%%%
%
\section{Implementation details}

There are a few important details with respect to the implementation of kriging methods in practice.

First of all, within this chapter, we have assumed that you always use all sample points to interpolate any point on the plane.
While this is optimal in theory, if a large number of sample points are used to interpolate every point, kriging can be \emph{very slow} in practice.
The reason for this is because matrix \(A\) will be very large, and inverting a matrix is a computationally expensive process.
Since the weights of far-away sample points are usually very small, the usual solution is to limit the number of sample points used, either by using a search radius, or by selecting only a given number of its closest sample points.
However, when the weights of far-away points are not negligible, this will cause artefacts in the final result.

Another related issue is that kriging is often said to be exact in theory, \ie\ it passes exactly through the sample points.
However, this is only true if the nugget of the theoretical variogram function is set to zero.
Some authors and implementations get around this by hard-coding \(\gamma(0) = 0\), but this can create a discontinuity between the value at the sample point and its immediate neighbourhood.

Finally, it is worth noting that kriging can be directly applied to any point on the plane, yielding a result such as the one in Figure~\ref{fig:interpolation}.
However, much like other interpolation methods, kriging is only reliable in the domain (\ie\ roughly the convex hull of the points).
It can extrapolate (often by using negative weights), but that does not mean that the results outside the domain are accurate.

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{figs/interpolation}
\caption{The result of using ordinary kriging to interpolate on a grid of points using the sample dataset using only the sample points within 15 units of each interpolated point.}%
\label{fig:interpolation}
\end{figure}

%%%
%
\section{Notes and comments}

\citet{Krige51} is the original publication by Danie Krige, which was later formalised by Georges Matheron~\citep{Matheron62,Matheron65}.
How this came to be is best explained in \citet{Cressie93}.

If you have trouble following the derivations of the kriging equations or want to know more about them, \citet{Lichtenstern13} explains this well.
If you feel like your statistics background is a bit weak, you first might want to have a look at \citet{Fewster14}, particularly Chapter~3.

A relatively simple explanation of kriging with agricultural examples is given by \citet{Oliver15}.
A standard reference textbook that is good but not so easy to follow is \citet{Wackernagel03}.
The mathematics covered in this chapter is partly based on the latter.

Pyinterpolate\marginnote{\url{https://pyinterpolate.readthedocs.io/}} is a good Python library to perform kriging and is used to generate some of the example figures from this chapter.

Two other good Youtube videos that explain kriging:
\begin{itemize}
\item \url{https://www.youtube.com/watch?v=CVkmuwF8cJ8}
\item \url{https://www.youtube.com/watch?v=98zz25kTteQ}
\end{itemize}

%%%
%
\section{Exercises}

\begin{enumerate}
\item Why can using a search radius create artefacts in the interpolated terrain?
\item If kriging generally provides better results than other interpolation methods, why would you use something else (eg IDW)?
\item What does a nugget of zero say about a dataset? What about a large nugget?
\item What kind of dataset would yield a flat variogram (\ie\ a horizontal line)?
\end{enumerate}
